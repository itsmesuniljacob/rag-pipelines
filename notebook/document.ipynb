{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fec16ad",
   "metadata": {},
   "source": [
    "## Data Ingestion   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b30368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66448371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Hello, world!' metadata={'source': 'example.pdf', 'pages': 1, 'author': 'Sunil J', 'date': '2021-01-01'}\n"
     ]
    }
   ],
   "source": [
    "# These metadata fields are optional and can be omitted. This is helpful for the RAG pipeline to track the source of the document,\n",
    "# Also for filtering the documents\n",
    "document = Document(\n",
    "    page_content=\"Hello, world!\", \n",
    "    metadata={\n",
    "        \"source\": \"example.pdf\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Sunil J\",\n",
    "        \"date\": \"2021-01-01\"  \n",
    "        }\n",
    "    )\n",
    "\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5653f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/koh_samet.txt'}, page_content='Koh Samet is a beautiful tropical island located about three hours from Bangkok, known for its stunning white sandy beaches, clear turquoise waters, and peaceful atmosphere. This small island offers a perfect escape with its lush jungles, rocky outcrops, fresh seafood, and a mix of relaxing bars and resorts, making it an ideal destination for both adventure and relaxation.\\n\\nIntroduction to Koh Samet\\n\\nKoh Samet is a charming island situated in the Gulf of Thailand, within Rayong province. It lies approximately 220 km from Bangkok and just 75 km from Pattaya, making it a popular weekend getaway for locals and travelers seeking a break from city life. Despite its proximity to major cities, Koh Samet remains a hidden gem, known for its idyllic beaches, fresh seafood, and laid-back vibe.\\n\\nGetting There\\n\\nFrom Bangkok to Ban Phe Pier: The journey to Koh Samet begins with a 2-3 hour trip from Bangkok to Ban Phe Pier in Rayong. Options include:\\n\\nPersonal car for flexibility\\nBus or minivan (recommended for affordability and comfort)\\nTaxi (expensive and not recommended)\\nTrain (economical but slow and less comfortable)\\nPlane (not practical due to distance from the pier)\\nFrom Ban Phe Pier to Koh Samet:\\n\\nSpeedboat: 15 minutes, 200 THB, operates 24 hours\\nFerry: 40 minutes, 70 THB, operates until 6 pm\\nTop Resorts and Accommodations\\n\\nParadee: Luxury resort near Ao Kiew with garden and beachfront cottages, some with roof decks and bathtubs. Price: ~17,000 THB/night.\\nSai Kaew Beach Resort: Located on Sai Kaew Beach, features 3 outdoor pools and 2 beachfront restaurants. Price: ~4,500 THB/night.\\nSamed Villa Resort: Cozy wooden Thai-style rooms near Ao Phai and Tubtim Beach, offers bike rentals and water sports. Price: ~1,700 THB/night.\\nLe Vimarn Cottages & Spa: Beach-style bungalows near Ao Prao Beach with a renowned spa and outdoor pool. Price: ~5,600 THB/night.\\nAo Prao Resort: Surrounded by tropical vegetation, offers hiking and biking trails, a wine cellar, and bar. Price: ~4,000 THB/night.\\nGetting Around Koh Samet\\n\\nWalking: Suitable for short distances around the main town but less practical for the whole island due to terrain.\\nMotorbike: Popular and flexible option; rental starts at 250 THB/day. Roads can be bumpy and mountainous in the south.\\nRod Song Thaew: Shared pickup truck taxis with fares between 20-100 THB, convenient for those who don’t drive.\\nExploring the Island\\n\\nUpon arrival at Nadan Pier, visitors are greeted by the statue of Pisuea Samet, a female giant from local legend. The island offers a variety of beaches, hiking trails, and cultural experiences, making it a perfect blend of natural beauty and local charm')]\n"
     ]
    }
   ],
   "source": [
    "### Using text loader to load the data from the file\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/koh_samet.txt\",encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c9fbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1465.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/koh_samet.txt'}, page_content='Koh Samet is a beautiful tropical island located about three hours from Bangkok, known for its stunning white sandy beaches, clear turquoise waters, and peaceful atmosphere. This small island offers a perfect escape with its lush jungles, rocky outcrops, fresh seafood, and a mix of relaxing bars and resorts, making it an ideal destination for both adventure and relaxation.\\n\\nIntroduction to Koh Samet\\n\\nKoh Samet is a charming island situated in the Gulf of Thailand, within Rayong province. It lies approximately 220 km from Bangkok and just 75 km from Pattaya, making it a popular weekend getaway for locals and travelers seeking a break from city life. Despite its proximity to major cities, Koh Samet remains a hidden gem, known for its idyllic beaches, fresh seafood, and laid-back vibe.\\n\\nGetting There\\n\\nFrom Bangkok to Ban Phe Pier: The journey to Koh Samet begins with a 2-3 hour trip from Bangkok to Ban Phe Pier in Rayong. Options include:\\n\\nPersonal car for flexibility\\nBus or minivan (recommended for affordability and comfort)\\nTaxi (expensive and not recommended)\\nTrain (economical but slow and less comfortable)\\nPlane (not practical due to distance from the pier)\\nFrom Ban Phe Pier to Koh Samet:\\n\\nSpeedboat: 15 minutes, 200 THB, operates 24 hours\\nFerry: 40 minutes, 70 THB, operates until 6 pm\\nTop Resorts and Accommodations\\n\\nParadee: Luxury resort near Ao Kiew with garden and beachfront cottages, some with roof decks and bathtubs. Price: ~17,000 THB/night.\\nSai Kaew Beach Resort: Located on Sai Kaew Beach, features 3 outdoor pools and 2 beachfront restaurants. Price: ~4,500 THB/night.\\nSamed Villa Resort: Cozy wooden Thai-style rooms near Ao Phai and Tubtim Beach, offers bike rentals and water sports. Price: ~1,700 THB/night.\\nLe Vimarn Cottages & Spa: Beach-style bungalows near Ao Prao Beach with a renowned spa and outdoor pool. Price: ~5,600 THB/night.\\nAo Prao Resort: Surrounded by tropical vegetation, offers hiking and biking trails, a wine cellar, and bar. Price: ~4,000 THB/night.\\nGetting Around Koh Samet\\n\\nWalking: Suitable for short distances around the main town but less practical for the whole island due to terrain.\\nMotorbike: Popular and flexible option; rental starts at 250 THB/day. Roads can be bumpy and mountainous in the south.\\nRod Song Thaew: Shared pickup truck taxis with fares between 20-100 THB, convenient for those who don’t drive.\\nExploring the Island\\n\\nUpon arrival at Nadan Pier, visitors are greeted by the statue of Pisuea Samet, a female giant from local legend. The island offers a variety of beaches, hiking trails, and cultural experiences, making it a perfect blend of natural beauty and local charm'), Document(metadata={'source': '../data/machine_learning.txt'}, page_content='Machine learning is a branch of artificial intelligence where computers use algorithms to learn patterns from data and improve their performance on tasks without being explicitly programmed. It enables systems to make predictions, recognize patterns, or generate content by training on large datasets.\\n\\nOverview of Machine Learning\\n\\nMachine learning (ML) is a subset of artificial intelligence (AI) focused on creating algorithms that learn from data patterns to make predictions or decisions without explicit programming.\\nIt is the foundation of many modern AI applications, including forecasting, autonomous vehicles, and generative AI models like large language models (LLMs).\\nThe core goal of ML is generalization: training a model on data so it performs well on new, unseen data.\\nHow Machine Learning Works\\n\\nML models learn by optimizing parameters to minimize errors on training data, enabling them to infer correct outputs for real-world tasks.\\nData must be represented numerically, often as vectors of features, which may require feature engineering to convert raw data (like text or images) into usable formats.\\nDeep learning, a subset of ML using deep neural networks, automates much of the feature extraction process and excels with large datasets and computational power.\\nTypes of Machine Learning\\n\\nSupervised Learning: Models learn from labeled data to predict outcomes, used in classification and regression tasks.\\nUnsupervised Learning: Models identify patterns or groupings in unlabeled data, useful for clustering and dimensionality reduction.\\nReinforcement Learning: Models learn to make decisions by receiving rewards or penalties from their environment, optimizing actions over time.\\nMachine Learning vs. Artificial Intelligence\\n\\nAll machine learning is AI, but not all AI is machine learning.\\nTraditional AI can be rule-based systems with explicit instructions, while ML systems learn patterns implicitly from data.\\nML offers greater flexibility and scalability for complex tasks where explicit programming is impractical.\\nApplications and Importance\\n\\nML powers many technologies such as spam filters, recommendation systems, medical diagnosis tools, and autonomous systems.\\nThe rise of big data and GPUs has accelerated the development and deployment of sophisticated ML models.\\nML is closely linked with data science, automating data analysis and enabling autonomous task execution.\\nThis summary provides a comprehensive understanding of machine learning, its mechanisms, types, and significance in modern technology.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Using directory loader to load the text files from the directory\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"../data\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    show_progress=True,\n",
    "    )\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3963d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/excel/top_10_countries_population.xlsx'}, page_content='Rank Country Population (millions) 1 China 1425 2 India 1417 3 United States 339 4 Indonesia 277 5 Pakistan 240 6 Nigeria 223 7 Brazil 216 8 Bangladesh 174 9 Russia 144 10 Mexico 130')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read my excel file from the data/excel folder using langchain excel loader\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader    \n",
    "\n",
    "loader = UnstructuredExcelLoader(\"../data/excel/top_10_countries_population.xlsx\")\n",
    "documents = loader.load()\n",
    "print(len(documents))\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1afe635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b93d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model all-MiniLM-L6-v2\n",
      "Error loading model all-MiniLM-L6-v2: \n",
      "AutoModel requires the PyTorch library but it was not found in your environment. Check out the instructions on the\n",
      "installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
      "Please note that you may need to restart your runtime after installation.\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Initialize embeddings manager\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m embeddings_manager = \u001b[43mEmbeddingManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m embeddings_manager\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mEmbeddingManager.__init__\u001b[39m\u001b[34m(self, model_name)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mself\u001b[39m.model_name = model_name\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mEmbeddingManager.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m loaded successfully. Embedding dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model.get_sentence_embedding_dimension()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/01-gitDownloads/02-build-rag-pipeline/.venv/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:327\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m._modules.values()):\n\u001b[32m    326\u001b[39m     sent_embedding_dim_method = \u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[33m\"\u001b[39m\u001b[33mget_sentence_embedding_dimension\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(sent_embedding_dim_method):\n\u001b[32m    328\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m sent_embedding_dim_method()\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/01-gitDownloads/02-build-rag-pipeline/.venv/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:2253\u001b[39m, in \u001b[36m_load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/01-gitDownloads/02-build-rag-pipeline/.venv/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:338\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/01-gitDownloads/02-build-rag-pipeline/.venv/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:87\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: Union[List[\u001b[38;5;28mstr\u001b[39m], List[Dict], List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]):\n\u001b[32m     85\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m    Tokenizes a text and maps tokens to token-ids\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     88\u001b[39m     output = {}\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(texts[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/01-gitDownloads/02-build-rag-pipeline/.venv/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:185\u001b[39m, in \u001b[36m_load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/01-gitDownloads/02-build-rag-pipeline/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:2142\u001b[39m, in \u001b[36m__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/01-gitDownloads/02-build-rag-pipeline/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:2128\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mImportError\u001b[39m: \nAutoModel requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"\n",
    "    This class is used to manage the embeddings for the documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading model {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model {self.model_name} loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, documents: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for the documents.\n",
    "        Args:\n",
    "            documents: List[str]\n",
    "        Returns:\n",
    "            np.ndarray of shape (n_documents, n_dimensions)\n",
    "        \"\"\"\n",
    "        print(f\"Generating embeddings for {len(documents)} documents\")\n",
    "        embeddings = self.model.encode(documents,show_progress_bar=True)\n",
    "        print(f\"Embeddings generated for {len(documents)} documents with shape {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "# Initialize embeddings manager\n",
    "embeddings_manager = EmbeddingManager()\n",
    "embeddings_manager\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e239d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
